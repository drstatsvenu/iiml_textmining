{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import traceback\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json2df(fname, year):\n",
    "    with open(fname, \"r\") as f_in:\n",
    "        orgData = json.load(f_in)\n",
    "    cleanTitles = {}\n",
    "    pattern = \"[0-9]+\\.\"\n",
    "    for k, v in orgData.items():\n",
    "        title = re.sub(pattern, \"\", k.lstrip().rstrip())\n",
    "        cleanTitles[title.lstrip().rstrip()] = v\n",
    "    df = pd.DataFrame(index=cleanTitles.keys())\n",
    "    for key in v.keys():\n",
    "        df[key] = \"\"\n",
    "    for ix, rowData in df.iterrows():\n",
    "        propData = cleanTitles[ix]\n",
    "        for k, v in propData.items():\n",
    "            if k != \"comments\":\n",
    "                rowData.loc[k] = v\n",
    "    df.columns = [c.lower().replace(\" \", \"_\").replace(\":\", \"\") for c in df]\n",
    "    df['n_comments'] = df.pop('comments')\n",
    "    for ix, rowdata in df.iterrows():\n",
    "        rowdata.loc[\"n_comments\"] = len(cleanTitles[ix]['comments'])\n",
    "    df['n_comments'] = df['n_comments'].astype(int)\n",
    "    df['n_votes'] = df['n_votes'].astype(int)\n",
    "    df['year'] = year\n",
    "    for col in df:\n",
    "        if df[col].dtype is np.dtype('O'):\n",
    "            df[col] = df.pop(col).apply(lambda x: x.lstrip().rstrip())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_request(url):\n",
    "    \"\"\"Make an HTTP GET request and return data in beautiful soup.\n",
    "\n",
    "    :param url: URL to be scraped.\n",
    "    \"\"\"\n",
    "    headers = {}\n",
    "    headers['User-Agent'] = (\"Mozilla/5.0 (Macintosh; Intel Mac\"\n",
    "                             \" OS X 10_11_5) AppleWebKit/537.36\"\n",
    "                             \"(KHTML, like Gecko) Chrome/51.0.\"\n",
    "                             \"2704.103 Safari/537.36\")\n",
    "    r = requests.get(url, headers=headers)\n",
    "    return bs(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(url, base_url=\"https://in.pycon.org\"):\n",
    "    result = {}\n",
    "    soup = make_request(url)\n",
    "    soup_proposals = soup.findAll(\n",
    "        'div', attrs={'class': 'row user-proposals'})\n",
    "    for proposal in soup_proposals:\n",
    "        p = proposal.find('h3', attrs={'class': 'proposal--title'})\n",
    "        title, url = p.text, \"\".join([base_url,\n",
    "                                      p.find('a').get('href', '')])\n",
    "        soup = make_request(url)\n",
    "        if not soup:\n",
    "            continue\n",
    "        soup_proposal = soup.findAll(\n",
    "            'div', attrs={'class': 'proposal-writeup--section'})\n",
    "        temp = {}\n",
    "        for data in soup_proposal:\n",
    "            try:\n",
    "                temp[data.find('h4').text] = \"\".join(\n",
    "                    [ptag.text for ptag in data.findAll('p')])\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "        temp['comments'] = []\n",
    "        for comment in soup.findAll(\"div\", attrs={'class': 'comment-description'}):\n",
    "            text = comment.find(\"span\")\n",
    "            if text:\n",
    "                temp['comments'].append(dict(\n",
    "                    text=text.text.strip(),\n",
    "                    by=comment.find('b').text.strip(),\n",
    "                    time=comment.find('small').text.strip()))\n",
    "        talk_details = soup.findAll('tr')\n",
    "        for section in talk_details:\n",
    "            row = section.findAll('td')\n",
    "            temp[row[0].text] = row[1].text\n",
    "        vCount = soup.findAll('h1', attrs={'class': 'vote-count'})[0]\n",
    "        temp[\"n_votes\"] = int(vCount.text.lstrip().rstrip())\n",
    "        result[title] = temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaidevd/anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/jaidevd/anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "cfp_2015_url = \"https://in.pycon.org/cfp/pycon-india-2015/proposals/\"\n",
    "cfp_2016_url = \"https://in.pycon.org/cfp/2016/proposals/\"\n",
    "results = scrape(cfp_2015_url)\n",
    "with open(\"cfp2015.json\", \"w\") as f_out:\n",
    "    json.dump(results, f_out)\n",
    "results = scrape(cfp_2016_url)\n",
    "with open(\"cfp2016.json\", \"w\") as f_out:\n",
    "    json.dump(results, f_out)\n",
    "df1 = json2df(\"cfp2015.json\", 2015)\n",
    "df2 = json2df(\"cfp2016.json\", 2016)\n",
    "df = pd.concat((df1, df2), axis=0)\n",
    "df.to_csv(\"cfp.tsv\", encoding=\"utf-8\", sep=\"\\t\", index_label=\"title\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
